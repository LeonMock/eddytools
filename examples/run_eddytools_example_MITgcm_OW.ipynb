{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import xmitgcm as xmit\n",
    "import convertMITgcmTime.convertMITgcmTime as cmt\n",
    "import xgcm\n",
    "import pickle\n",
    "import eddytools as et\n",
    "from cmocean import cm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "# Obviously it is not a great idea to ignore warnings, however there are quite many\n",
    "# RuntimeWarnings because of division by 0 in some parts of this notebook. To keep\n",
    "# the instructive nature of this example notebook, these warnings are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = LocalCluster(n_workers=3, threads_per_worker=2)\n",
    "#client = Client(cluster)\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "datapath = '/home/jan/Data/MITgcm_test_data/'\n",
    "gridpath = '/home/jan/Data/MITgcm_test_data/grid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "In case the MITgcm output is in binary format (`*.meta` and `*.data`), use `xmitgcm` to load the data. If the data is in `netCDF` format, make sure that you define all the necessary grid information for `xgcm` to work properly! Because the time dimension of the MITgcm model calendar does not really contain information about the calendar used, we create a new time dimension that can be understood by `xarray` with [`convertMITgcmTime`](https://github.com/jk-rieck/convertMITgcmTime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "start = 1555200\n",
    "end = 1580760\n",
    "step = 360\n",
    "dt = 1200.\n",
    "data = xmit.open_mdsdataset(datapath, iters=list(np.arange(start, end + 1, step)), delta_t=dt,\n",
    "                            ignore_unknown_vars=True, grid_dir=gridpath, geometry=\"cartesian\", calendar=\"model\", prefix=\"3D_diags_5d\")\n",
    "data = data.chunk({'time': 34, 'Z': 55, 'Zl': 55, 'Zu': 55, 'Zp1': 56, 'XC': 250, 'XG': 250, 'YC': 300, 'YG': 300})\n",
    "data[\"time\"] = cmt.convert2cftime360(data.time.dt.days.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics for xgcm (not strictly necessary)\n",
    "data['drW'] = data.hFacW * data.drF #vertical cell size at u point\n",
    "data['drS'] = data.hFacS * data.drF #vertical cell size at v point\n",
    "data['drC'] = data.hFacC * data.drF #vertical cell size at tracer point\n",
    "data = data.set_coords([\"drW\", \"drS\", \"drC\"])\n",
    "\n",
    "metrics = {\n",
    "    ('X'): ['dxC', 'dxG', 'dxF', 'dxV'], # X distances\n",
    "    ('Y'): ['dyC', 'dyG', 'dyF', 'dyU'], # Y distances\n",
    "    ('Z'): ['drF', 'drW', 'drS', 'drC'], # Z distances\n",
    "    ('X', 'Y'): ['rAw', 'rAs', 'rA', 'rAz'] # Areas in x-y plane\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OKUBO-WEISS\n",
    "We use xgcm to define the grid for the interpolation and differentiation necessary for the calculation of vorticity and the Okubo-Weiss parameter.  \n",
    "For more information on the Okubo-Weiss parameter and its calculation see [Okubo, 1970](https://doi.org/10.1016/0011-7471(70)90059-8); [Weiss, 1991](https://doi.org/10.1016/0167-2789(91)90088-Q); [Basdevant and Philipovitch, 1994](https://doi.org/10.1016/0167-2789(94)90222-4); [Chelton et al., 2007](https://doi.org/10.1029/2007GL030812)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = xgcm.Grid(data, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate vorticity and Okubo-Weiss parameter\n",
    "data_OW = et.okuboweiss.calc(data, grid, 'UVEL', 'VVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the new variables `OW` and `vort` to the dataset `data` and make sure the chunk sizes are as before.\n",
    "data = xr.merge([data, data_OW], compat='override').chunk({'XC': 250, 'XG': 250, 'YC': 300, 'YG': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERPOLATION\n",
    "Now we interpolate the data. If the data is entirely on a grid, that has one-dimensional latitude and longitude dimensions (i.e. longitude at one x-index does not change with changing y-index and latitude at one y-index does not change with changing x-index), the fields are simply interpolated to the F-point of the grid (vorticity and OW paramter are calculated and stored already on the F-point).\n",
    "If the grid is curivlinear, e.g. North of 20S or so on the ORCA grids, a regridding is performed.  \n",
    "Note that the interpolation can take VERY long for large region and/or long time periods. If one has to deal with problems regarding the time it takes, one could split the interpolation into several parts, let's say interpolate one year at a time, store the results and later load the combined interpolated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longitude in all the parameter definitions (`lon1` and `lon2`) needs to be in the range (-180, 180) for `latlon` grids. Longitudes larger then 180 will result in an error. For cartesian grids, longitudes must be positive with meter as units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT:  \n",
    "The region that gets interpolated needs to be larger than the region used later for detection and tracking. This is required to ensure that eddies close to the boundaries of the detection region can be detected, tracked and sampled, even if their surroundings extend beyond the limits of the detection region.  \n",
    "To handle this, the region is internally extended by 2 degrees (for a `latlon` grid) or 200 km (for a cartesian grid) in each longitude direction and 1 degree (`latlon`) or 100km (cartesian) in latitude direction. So you need to make sure that the region you specify is not closer to the limits of your data that that.  \n",
    "If you want to detect eddies in the region 140-170E and 50-60S, the interpolation will be carried out within 138-172E and 49-61S! However, you still need to define 140-170E and 50-60S in the `interpolation_parameters` (or something equivalent for the cartesian grids).  \n",
    "\n",
    "Before the interpolation, we need to define a ocean mask at the F-point, as the MITgcm does not write this as output by default. For that we interpolate the ocean mask at the V point (`maskS`) in X direction, to shift it onto the F-point, then we decide that the mask should define ocean everywhere, where it is not zero. I.e. if an ocean point (`1`) and a land point (`0`) are interpolated (=`0.5`), this is converted to ocean (`1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add maskZ for interpolation (mask at F point)\n",
    "data['maskZ'] = grid.interp(data['maskS'], \"X\", to=\"left\", metric_weighted=[\"X\", \"Y\"])\n",
    "data['maskZ'] = data['maskZ'].where(data['maskZ']==1, other=0)\n",
    "data = data.set_coords([\"maskZ\"]).chunk({'XC': 250, 'XG': 250, 'YC': 300, 'YG': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not 100% sure yet how big the region will be in which you want to detect the eddies, you can specify a larger region for the interpolation and then later define smaller regions for the detection, tracking, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the interpolation\n",
    "interpolation_parameters = {'model': 'MITgcm',\n",
    "                            'grid': 'cartesian',\n",
    "                            'start_time': '0061-01-01', # time range start\n",
    "                            'end_time': '0061-12-30', # time range end\n",
    "                            'calendar': '360_day', # calendar, must be either 360_day or standard\n",
    "                            'lon1': 0.6e6, # minimum longitude of detection region\n",
    "                            'lon2': 1.3e6, # maximum longitude\n",
    "                            'lat1': 1.0e6, # minimum latitude\n",
    "                            'lat2': 1.3e6, # maximum latitude\n",
    "                            'res': 10., # resolution of the fields in km\n",
    "                            'vars_to_interpolate': ['OW', 'vort', 'THETA'], # variables to be interpolated \n",
    "                            'mask_to_interpolate': ['maskZ', 'maskC', 'Depth']} # masks to interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = et.interp.horizontal(data, metrics, interpolation_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be confused that interpolation of `dxV` and `dyU` is done although we did not request that. `maskZ`, `dxV` and `dyU` are always interpolated, even if you do not request that because they are essential for the detection of the eddies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold for the Okubo-Weiss parameter\n",
    "Some sort of threshold needs to be defined for the Okubo-Weiss parameter to distinguish regions within and outside of eddies. This is usually defined as `OW_thr = a * sigma` ([Chelton et al., 2007](https://doi.org/10.1029/2007GL030812)), where `sigma` is the spatial standard deviation of the Okubo-Weiss parameter and `a` is some factor (more on the factor later).  \n",
    "In regions with highly variable dynamic conditions, detection based on a single values of this standard deviation often fails to detect eddies in more \"quiet\" areas of the ocean ([Isern-Fontanet et al., 2003](https://doi.org/10.1175/1520-0426(2003)20<772:IOMEFA>2.0.CO;2)).Thus we here define a spatially variable threshold for the Okubo-Weiss parameter by calculating the (2D) spatial standard deviation in moving boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `OW` into memory so the `.rolling` operation is faster\n",
    "OW_tmp = data_int['OW'].sel(z=100, method=\"nearest\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert all land values to `NaN` so we don't have a lot of zeros when calculating\n",
    "# the standard deviation\n",
    "OW_tmp = OW_tmp.where(OW_tmp != 0)\n",
    "lon_tmp = OW_tmp['lon'].where(OW_tmp['lon'] > 0, other=OW_tmp['lon'] + 360.)\n",
    "OW_tmp = OW_tmp.assign_coords({'lon': lon_tmp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_OW_spatial_std = OW_tmp.rolling(\n",
    "                          lon=100, center=True, min_periods=1\n",
    "                          ).std(skipna=True).rolling(\n",
    "                          lat=100, center=True, min_periods=1\n",
    "                          ).std(skipna=True).mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use the mean spatial standard deviation of the Okubo-Weiss parameter over the whole considered region. To do that execute the cell below instead of the one above! Please note that `'OW_thr'` in `detection_parameters` needs to be defined differently, depending on which method of calculating the standard deviation you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_OW_spatial_std = OW_tmp.std(('lon', 'lat'), skipna=True).mean('time') # uncomment if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if OW_std is 2D\n",
    "data_int = data_int.update({'OW_std': (['lat', 'lon'], mean_OW_spatial_std.values)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if OW_std is 1D\n",
    "#data_int = data_int.update({'OW_std': mean_OW_spatial_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the interpolated field on disk so we do not have to do the interpolation again\n",
    "data_int.to_netcdf(datapath + 'test_data.nc', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we already did the interpolation earlier and now start off with the interpolated fields,\n",
    "# we just uncomment the line below to load them\n",
    "data_int = xr.open_dataset(datapath + 'test_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the fields\n",
    "plt.pcolormesh(data_int['OW'].isel(time=40, z=20), vmin=-1e-10, vmax=1e-10, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(data_int['OW_std'], vmin=-1e-10, vmax=1e-10, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several eddy-like features that have a low OW-parameter, now let's see whether the algorithm detects them as eddies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note for the setting of `detection_parameters`:  \n",
    "1. `'lon1'` and `'lon2'` need to be positive and in meters. \n",
    "2. `'OW_thr': data_int` if the field `OW_std` in `data_int` is 2D, i.e. we have a spatially varying standard deviation of the Okubo-Weiss parameter. `'OW_thr': data_int['OW_std'].values` if the field `OW_std` in `data_int` is 1D.  \n",
    "3. `'OW_thr_factor'` is usually chosen in the range `(-0.5, -0.2)` ([Chelton et al., 2007](https://doi.org/10.1029/2007GL030812); [Isern-Fontanet et al., 2003](https://doi.org/10.1175/1520-0426(2003)20<772:IOMEFA>2.0.CO;2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for eddy detection\n",
    "detection_parameters = {'model': 'MITgcm',\n",
    "                        'grid': 'cartesian',\n",
    "                        'start_time': '0061-01-01', # time range start\n",
    "                        'end_time': '0061-12-30', # time range end\n",
    "                        'calendar': '360_day', # calendar, must be either 360_day or standard\n",
    "                        'lon1': 0.6e6, # minimum longitude of detection region\n",
    "                        'lon2': 1.3e6, # maximum longitude\n",
    "                        'lat1': 1.0e6, # minimum latitude\n",
    "                        'lat2': 1.3e6, # maximum latitude\n",
    "                        'res': 10., # resolution of the fields in km\n",
    "                        'min_dep': 1000, # minimum ocean depth where to look for eddies in m\n",
    "                        'OW_thr': data_int, # \n",
    "                        'OW_thr_name': 'OW_std', # Okubo-Weiss threshold for eddy detection\n",
    "                        'OW_thr_factor': -0.3, # Okubo-Weiss parameter threshold\n",
    "                        'Npix_min': 20, # minimum number of pixels (grid cells) to be considered as eddy\n",
    "                        'Npix_max': 500} # maximum number of pixels (grid cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We detect eddies at a depth of 100 m\n",
    "eddies = et.detection.detect_OW(data_int.sel(z=100, method='nearest'), detection_parameters, 'OW', 'vort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this format, information for every detected eddy is stored\n",
    "# (eddies[t][ed], where t is the time step and ed the eddy number)\n",
    "eddies[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single time step to see what the detection method detected\n",
    "t=40\n",
    "d_p = detection_parameters\n",
    "plot_lon = data_int['lon'].where(data_int['lon'].values > 0, other=data_int['lon'].values + 360)\n",
    "plt.pcolormesh(plot_lon, data_int['lat'].values, data_int.OW.isel(time=t).sel(z=100, method='nearest').values,\n",
    "               vmin=-1e-10, vmax=1e-10, cmap=cm.balance)\n",
    "for c, l in zip(['whitesmoke', 'dimgray'], ['-', '--']):\n",
    "    plt.plot([d_p['lon1'], d_p['lon2'], d_p['lon2'], d_p['lon1'], d_p['lon1']],\n",
    "             [d_p['lat1'], d_p['lat1'], d_p['lat2'], d_p['lat2'], d_p['lat1']],\n",
    "             color=c, linestyle=l)\n",
    "\n",
    "for i in np.arange(0, len(eddies[t])-1):\n",
    "    if eddies[t][i]['lon'] < 0:\n",
    "        eddy_lon = eddies[t][i]['lon'] + 360\n",
    "    else:\n",
    "        eddy_lon = eddies[t][i]['lon']\n",
    "    for s, c in zip([10, 8], ['k', 'gold']):\n",
    "        plt.plot(eddy_lon, eddies[t][i]['lat'], marker='o', color=c, markersize=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all features below a certain OW threshold have been detected. Some of them, we might not consider as an eddy. If you have the feeling, that two many features are detected, that are not eddies, change parameters like `OW_thr_factor`, `Npix_min`, `Npix_max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every time step, we store one file on disk with all the information of the detected\n",
    "# eddies at this time step\n",
    "for i in np.arange(0, len(eddies)):\n",
    "    datestring = str(eddies[i][0]['time'])[0:10]\n",
    "    with open(datapath + 'test_'\n",
    "          + str(datestring) + '_eddies_OW0.3_test.pickle', 'wb') as f:\n",
    "        pickle.dump(eddies[i], f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRACKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note for the setting of `tracking_parameters`:  \n",
    "1. `'start_time'` is required to be no earlier than the earliest actual date of the detected eddies. In our case here, for the year 0002 and a 5-day temporal resolution of the data, this is `'0002-01-05'` (The `MITgcm` stores the 5-daily averages at the end of the 5-day period).\n",
    "2. `'lon1'` and `'lon2'` need to be identical to `'lon1'` and `'lon2'` in `detection_parameters`.  \n",
    "3. If you stored the detected eddies in files and want to track these, set `'dict': 0` and make sure `'data_path'`, `'file_root'` and `'file_spec'` are set accordingly. The method will look for files `datapath + file_root + 'YYYYMMDD' + file_spec + '.pickle'`, the date `'YYYYMMDD'` is automatically calculated from `'start_time'`, `'dt'`, and `'end_time'`. You have to make sure that the stored, detected eddies contain that date in their filename (e.g. as defined in the cell above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for eddy tracking\n",
    "tracking_parameters = {'model': 'MITgcm',\n",
    "                       'grid': 'cartesian',\n",
    "                       'start_time': '0061-01-03', # time range start\n",
    "                       'end_time': '0061-12-30', # time range end\n",
    "                       'calendar': '360_day', # calendar, must be either 360_day or standard\n",
    "                       'dt': 5, # temporal resolution of the data in days\n",
    "                       'lon1': 0.6e6, # minimum longitude of detection region\n",
    "                       'lon2': 1.3e6, # maximum longitude\n",
    "                       'lat1': 1.0e6, # minimum latitude\n",
    "                       'lat2': 1.3e6, # maximum latitude\n",
    "                       'res': 10.,\n",
    "                       'dE': 50., # maximum distance of search ellipsis from eddy center in towards the east \n",
    "                                 # (if set to 0, it will be calculated as (150. / (7. / dt)))\n",
    "                       'eddy_scale_min': 0.5, # minimum factor by which eddy amplitude and area are allowed to change in one timestep\n",
    "                       'eddy_scale_max': 1.5, # maximum factor by which eddy amplitude and area are allowed to change in one timestep\n",
    "                       'dict': eddies, # dictionary containing detected eddies to be used when not stored in files (set to 0 otherwise)\n",
    "                       'data_path': datapath, # path to the detected eddies pickle files\n",
    "                       'file_root': 'test',\n",
    "                       'file_spec': 'eddies_OW0.3_test',\n",
    "                       'ross_path': datapath} # path to rossrad.dat containing Chelton et a1. 1998 Rossby radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we track the eddies, all information needed has to be added to `tracking_parameters`\n",
    "tracks = et.tracking.track(tracking_parameters, in_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have tracked all eddies that met the criteria specified in `tracking_parameters`. Every entry `i` in `tracks[i]` corresponds to one complete track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entries in `track` look like this\n",
    "tracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a look at how the tracking performs, just pick an eddy and see whether it is tracked.\n",
    "ed1 = 1\n",
    "t = 0\n",
    "j = 4\n",
    "\n",
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "for i in np.arange(0, j):\n",
    "    plt.subplot(1, j, i + 1)\n",
    "    plt.pcolormesh(data_int.lon, data_int.lat, data_int.OW.sel(time=tracks[ed1]['time'][i]).sel(z=100, method='nearest').values,\n",
    "                   vmin=-1e-9, vmax=1e-9, cmap=cm.balance, shading='auto')\n",
    "    plt.plot(tracks[ed1]['lon'][t:t + i+1], tracks[ed1]['lat'][t:t + i+1], marker='o', color='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the tracks for later use\n",
    "with open(datapath\n",
    "          + 'test_00610101_00611230_tracks_OW0.3'\n",
    "          + '_test.pickle', 'wb') as f:\n",
    "    pickle.dump(tracks, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to open the tracks-file again (no need to do that if we just saved it)\n",
    "with open(datapath\n",
    "          + 'test_00610101_00611230_tracks_OW0.3'\n",
    "          + '_test.pickle', 'rb') as f:\n",
    "    tracks = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note for the setting of `sample_parameters`:  \n",
    "1. `'start_time'` is required to be no earlier than the earliest actual date of the tracked eddies. In our case here, for the year 0002 and a 5-day temporal resolution of the data, this is `'0002-01-05'` (The `MITgcm` stores the 5-daily averages at the end of the 5-day period).\n",
    "2. `'lon1'` and `'lon2'` need to be identical to `'lon1'` and `'lon2'` in `detection_parameters`.  \n",
    "3. Right now, the usage of `'range'` and `'split'` has not been thouroughly tested! It seems to work for most cases though.  \n",
    "\n",
    "`'range'`: Set to `True` if you only want to sample eddies within a certain range `'values_range'` of a property `'var_range'` in the dataset `'ds_range'`. `'var_range'` needs to be 2D (thus the `.isel(z=9)` in the example below) and interpolated to the same grid as `OW` used above. It is most likely that, if you follow this example, `'var_range'` is stored in the same dataset as `OW`. In the example below, only eddies that have a center temperature between 4 and 7 degrees C at depth level 10 (`z=9`) will be sampled and stored.  \n",
    "\n",
    "`'split'`: Set to `True` if you want to split the sampled eddies into two categories, above and below a certain threshold value `'value_split'` of a variable `'var_split'` in the dataset `'ds_split'`. As for `'range'`, `'var_split'` needs to be 2D and interpolated to the same grid as `OW` used above. In the example below the eddies will be put into two categories: In the first category, the eddies must have a center salinity above 34.0 and in the second category, below 34.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sampling we again have to specify some parameters, defining when, where and which \n",
    "# eddies to sample.\n",
    "# Because the dataset containing the sampled eddies can grow huge for larger regions and/or\n",
    "# longer time periods, `eddytools.sample.sample()` writes the samples for each individual\n",
    "# eddy to individual netcdf-files on disk!\n",
    "sample_parameters = {'model': 'MITgcm',\n",
    "                     'grid': 'cartesian',\n",
    "                     'start_time': '0061-01-05', # time range start\n",
    "                     'end_time': '0061-12-30', # time range end\n",
    "                     'max_time': 146, # maximum length of tracks to consider\n",
    "                                     # (model time steps)\n",
    "                     'calendar': '360_day', # calendar, must be either 360_day or standard\n",
    "                     'dt': 5, # temporal resolution of the data in days\n",
    "                     'lon1': 0.6e6, # minimum longitude of detection region\n",
    "                     'lon2': 1.3e6, # maximum longitude\n",
    "                     'lat1': 1.0e6, # minimum latitude\n",
    "                     'lat2': 1.3e6, # maximum latitude\n",
    "                     'res': 10.,\n",
    "                     'type': 'anticyclonic', # type of eddy\n",
    "                     'lifetime': 5, # length of the eddy's track in days\n",
    "                     'size': 25, # eddy size (diameter in km)\n",
    "                     'range': False, # sample eddy within a range of `var_range`\n",
    "                     'ds_range': data_int.isel(z=9), # dataset of `var_range`\n",
    "                     'var_range': ['THETA'], # variable to base the range on\n",
    "                     'value_range': [[4, 7],], # range of `var_range`\n",
    "                     'split': False, # split eddies at a threshold in below and above\n",
    "                     'ds_split': data_int.isel(z=9), # dataset of `var_split`\n",
    "                     'var_split': ['SALT'], # variable to base split on\n",
    "                     'value_split': [34.0,], # split eddies at this value\n",
    "                     'sample_vars': ['THETA'], # variables to sample\n",
    "                     'save_location': datapath, # where to store the netcdf files \n",
    "                     'save_name': 'test'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling can take quite long as for every eddy that fits the criteria we need to read data from disk at every time step  \n",
    "The names of the files will be defined by `save_name`, the criteria you specify and the eddy number  \n",
    "For this sampling parameters, the file name of the first eddy will be  \n",
    "`test.anticyclonic.larger_25.longer_5.0000001.nc`  \n",
    "\n",
    "If you set `'split'` to `True`, the file names will differ for the two categories.  \n",
    "`test.anticyclonic.larger_25.longer_5.0000001.above_thr.nc` for eddies that are above `'value_split'`  \n",
    "`test.anticyclonic.larger_25.longer_5.0000001.below_thr.nc`\n",
    "for eddies that are below `'value_split'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.sample.sample(tracks, data_int, sample_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now average over the sampled eddies.  \n",
    "As each eddy has its own file, we first need to find out how many files/samples there are, so we can loop over them and then store the datasets in a dictionary.  \n",
    "Note that for large region, i.e. a lot of samples the resulting dataset could be too large to fit into memory. The `chunks={}` argument to `xr.open_dataset()` is an attempt to overcome this, however I do not know yet whether this has a lot of effect! One could also split the samples into several parts and then work on each part seperately (the number of eddies going into each average is stored, so one could later do a weighted average over the different parts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out number of samples\n",
    "samples = {}\n",
    "num_samples = len(glob(sample_parameters['save_location']\n",
    "                       + sample_parameters['save_name'] + '.anticyclonic.larger_25.longer_5.*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load each stored sample\n",
    "for i in np.arange(1, num_samples + 1):\n",
    "    snum = \"%07d\" % (i,)\n",
    "    samples[i] = xr.open_dataset(sample_parameters['save_location'] + sample_parameters['save_name'] +\n",
    "                                 '.anticyclonic.larger_25.longer_5.' + str(snum) + '.nc', chunks={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a \"preparation\" is performed: Basically, all eddies are interpolated onto a normalized (in length) section crossing them zonally through the eddy center.  \n",
    "For every variable specified, at every time step, the values and anomalies (with respect to surroundings) are interpolated onto the normalized section and stored according to the month the eddy was first detected. The depth profile of the surroundings is stored as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_eddies = et.average.prepare(samples, ['THETA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_eddies['THETA_anom']['12'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for example, 3 eddies originating in December have been stored with a maximum length of 14 time steps. 55 is the length of the depth dimension, 101 the length of the normalized section (-0.5, 0.5, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can average these samples in different ways.  \n",
    "Three possibilities are given with `average`  \n",
    "1. seasonal -> bins the samples (of all available years) into four seasons (DJF, MAM, JJA, SON) and averages them into a seasonal climatology  \n",
    "2. monthly  -> does the same for each month (so you end up with a monthly climatology)  \n",
    "3. total    -> averages over all available eddies  \n",
    "The three methods all return the means, standard deviations (across eddies), and the number of eddies that went into the derived quantities for a) the variables specified, b) their anomalies to the surroundings, c) the surroundings. These results are stored in the output dictionary under `output['ave']['mean'][period]['variable']`, `output['ave']['mean'][period]['variable_anom']`, `output['ave']['mean'][period]['variable_around']`, respectively. `period` refers to 1. `DJF`, `MAM`, `JJA` or `SON`, 2. `01`, `02`, `03`, etc., representing the month, or in case of the total average thie layer in the dictionary does not exist.  \n",
    "Additionally, the averaged temporal evolution of the eddy centers will be stored under `output['evo'][...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_eddies = et.average.seasonal(normalized_eddies, ['THETA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(season_eddies['ave']['mean']['DJF']['THETA_anom'], vmin=-1, vmax=1, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_eddies = et.average.monthly(normalized_eddies, ['THETA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(monthly_eddies['ave']['mean']['03']['THETA_anom'], vmin=-1, vmax=1, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eddies = et.average.total(normalized_eddies, ['THETA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(total_eddies['ave']['mean']['THETA_anom'], vmin=-1, vmax=1, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_eddy_new]",
   "language": "python",
   "name": "conda-env-py3_eddy_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
