{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import xgcm\n",
    "from xorca.lib import load_xorca_dataset\n",
    "import pickle\n",
    "import eddytools as et\n",
    "from cmocean import cm\n",
    "from scipy.signal import convolve\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "# Obviously it is not a great idea to ignore warnings, however there are quite many\n",
    "# RuntimeWarnings because of division by 0 in some parts of this notebook. To keep\n",
    "# the instructive nature of this example notebook, these warnings are ignored.\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection parameters\n",
    "Npix_min = 20*6*5\n",
    "Npix_max = 500*6*5\n",
    "OW_thr_factor =-0.2\n",
    "\n",
    "sigma = 9 # smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestart, dateend = \"2012-04-10\", \"2012-05-04\"\n",
    "#periods = [\n",
    "#    (\"2012-01-01\", \"2012-01-25\"), (\"2012-01-26\", \"2012-02-19\"), (\"2012-02-20\", \"2012-03-15\"),\n",
    "#    (\"2012-03-16\", \"2012-04-09\"), (\"2012-04-10\", \"2012-05-04\"), (\"2012-05-05\", \"2012-05-29\"),\n",
    "#    (\"2012-05-30\", \"2012-06-28\"), (\"2012-10-27\", \"2012-11-20\"), (\"2012-11-21\", \"2012-12-15\"),\n",
    "#    \n",
    "#    (\"2012-06-29\", \"2012-07-28\"), (\"2012-07-29\", \"2012-08-27\"), (\"2012-08-28\", \"2012-09-26\"), (\"2012-09-27\", \"2012-10-26\"),\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth= 0  #corresponding to... \n",
    "depth_index = 0 # tracking only on surface\n",
    "\n",
    "#mesh_mask = xr.open_dataset('/gxfs_work/geomar/smomw355/model_data/ocean-only/INALT60.L120-KRS0020/nemo/suppl/2_INALT60.L120-KRS0020_mesh_mask.nc') \n",
    "#depth_information = [(round(mesh_mask.nav_lev.values[i]), i) for i in range(0, 64, 3)] #upper 1000m, otherwise: range(0, len(mesh_mask.nav_lev.values), 3)\n",
    "#print(depth_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'INALT60.L120-KRS0020'\n",
    "data_resolution = '1d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TRACKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note for the setting of `tracking_parameters`:  \n",
    "1. `'start_time'` is required to be no earlier than the earliest actual date of the detected eddies. In our case here, for the year 0002 and a 5-day temporal resolution of the data, this is `'0002-01-05'` (The `MITgcm` stores the 5-daily averages at the end of the 5-day period).\n",
    "2. `'lon1'` and `'lon2'` need to be identical to `'lon1'` and `'lon2'` in `detection_parameters`.  \n",
    "3. If you stored the detected eddies in files and want to track these, set `'dict': 0`, make sure `'data_path'`, `'file_root'` and `'file_spec'` are set accordingly and use `in_file=True` as an argument to `et.tracking.track()`.. The method will look for files `datapath + file_root + 'YYYYMMDD' + file_spec + '.pickle'`, the date `'YYYYMMDD'` is automatically calculated from `'start_time'`, `'dt'`, and `'end_time'`. You have to make sure that the stored, detected eddies contain that date in their filename (e.g. as defined in the cell above)!  \n",
    "\n",
    "Some notes on the search distance `search_dist` that is used to determine in what radius to look for a similar eddy at the next time step.  \n",
    "    - If `search_circle: True`, the algorithm simply searches for similar eddies within a radius of `search_dist` kilometers around the center of the current eddy. This is the simplest method.\n",
    "    - If `search_circle: False`, the algorithm will determine where to look for similar eddies based on an ellipse with a minor axis of `search_dist` kilometers. If `search_dist: 0`, the minor and major axes of the ellipse are calculated based on the propagation of Rossby waves to account for the fact that eddies will move towards the West (see [Chelton et al., 2011](https://www.sciencedirect.com/science/article/pii/S0079661111000036) for details). In regions with strong currents this might lead to a loss of a lot of tracks, but no sensitivity studies have been conducted... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = f'/gxfs_work/geomar/smomw523/eddytools/results/{experiment_name}/smoothed/{sigma}/{data_resolution}/depth-{depth}/'   ## !! SMOOTHED !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for eddy tracking\n",
    "tracking_parameters = {'model': 'ORCA',\n",
    "                       'grid': 'latlon',\n",
    "                       'start_time': datestart, # time range start\n",
    "                       'end_time': dateend, # time range end\n",
    "                       'calendar': 'standard', # calendar, must be either 360_day or standard\n",
    "                       'dt': 1, # temporal resolution of the data in days\n",
    "                       'lon1': 0, # minimum longitude of detection region\n",
    "                       'lon2': 40, # maximum longitude\n",
    "                       'lat1': -45, # minimum latitude\n",
    "                       'lat2': -25, # maximum latitude\n",
    "                       'search_dist': 50., # maximum distance of search ellipse/circle from eddy center in km\n",
    "                                          # if ellipse: towards the east (if set to 0, it\n",
    "                                          # will be calculated as (150. / (7. / dt)))\n",
    "                       # max vel 3.5 m/s --> 50km in 4 hours\n",
    "                       'search_circle': True, # if True, search in a circle. otherwise use ellipse\n",
    "                       'eddy_scale_min': 0.5, # minimum factor by which eddy amplitude and area are allowed to change in one timestep\n",
    "                       'eddy_scale_max': 1.5, # maximum factor by which eddy amplitude and area are allowed to change in one timestep\n",
    "                       'dict': 0, # eddies dictionary containing detected eddies to be used when not stored in files (set to 0 otherwise)\n",
    "                       'data_path': datapath, # path to the detected eddies pickle files\n",
    "                       'file_root': 'Eddies',\n",
    "                       'file_spec': f'OW{np.abs(OW_thr_factor)}_Npix-{Npix_min}-{Npix_max}',\n",
    "                       'ross_path': '/gxfs_work/geomar/smomw523/eddytools/'} # path to rossrad.dat containing Chelton et a1. 1998 Rossby radii\n",
    "\n",
    "# detected eddies are loaded from file with the filename\n",
    "# 'trac_param['data_path'] + trac_param['file_root'] + '_'\n",
    "# + str(datestring) + '_' + trac_param['file_spec'] + '.pickle'\n",
    "# `datestring` is created from the `time` value of the eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no eddies found to track\n"
     ]
    }
   ],
   "source": [
    "# Now we track the eddies, all information needed has to be added to `tracking_parameters`\n",
    "tracks = et.tracking.track(tracking_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have tracked all eddies that met the criteria specified in `tracking_parameters`. Every entry `i` in `tracks[i]` corresponds to one complete track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The entries in `track` look like this\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtracks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# The entries in `track` look like this\n",
    "tracks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a look at how the tracking performs, just pick two eddies and see whether they are tracked.\n",
    "ed1 = 0\n",
    "ed2 = 4\n",
    "t = 0\n",
    "j = 5\n",
    "\n",
    "plt.figure(figsize=(18, j))\n",
    "\n",
    "plot_lon = data_int['lon'].where(data_int['lon'].values > 0, other=data_int['lon'].values + 360)\n",
    "\n",
    "ed1_lon = tracks[ed1]['lon']\n",
    "ed1_lon[ed1_lon < 0] = ed1_lon[ed1_lon < 0] + 360\n",
    "ed2_lon = tracks[ed2]['lon']\n",
    "ed2_lon[ed2_lon < 0] = ed2_lon[ed2_lon < 0] + 360\n",
    "\n",
    "for i in np.arange(0, j):\n",
    "    plt.subplot(1, j, i + 1)\n",
    "    plt.pcolormesh(plot_lon, data_int.lat, data_int.OW.isel(time=t + i).values,\n",
    "                   vmin=-5e-10, vmax=5e-10, cmap=cm.balance, shading='auto')\n",
    "    plt.plot(ed1_lon[t:t + i+1], tracks[ed1]['lat'][t:t + i+1], marker='o', color='m')\n",
    "    plt.plot(ed2_lon[t:t + i+1], tracks[ed2]['lat'][t:t + i+1], marker='o', color='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We save the tracks for later use\n",
    "with open(datapath\n",
    "          + 'test_19760101_19761231_tracks_OW0.3'\n",
    "          + '_test.pickle', 'wb') as f:\n",
    "    pickle.dump(tracks, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to open the tracks-file again (no need to do that if we just saved it)\n",
    "with open(datapath\n",
    "          + 'test_19760101_19761231_tracks_OW0.3'\n",
    "          + '_test.pickle', 'rb') as f:\n",
    "    tracks = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split up the tracking\n",
    "If detecting and tracking in larger regions, the tracking can take very long. Because computations are often limited in duration on high-performance computing clusters, an option to split up the tracking into several chunks has been added. Note that the tracking still needs to be done serially, one chunk after the other.\n",
    "\n",
    "We first track the first half of the month, using the function `split_track()`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tracking_parameters_split = tracking_parameters\n",
    "tracking_parameters_split['start_time'] = '1976-01-03'\n",
    "tracking_parameters_split['end_time'] =  '1976-01-15'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_split1, tracks_split1, terminated1 = et.tracking.split_track(tracking_parameters_split, \n",
    "                                                                 in_file=False, continuing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `tmp_split1` contains the tracks for the first time span as if they were tracked with `track()`. `tracks_split` additionally contains all unfinished tracks and needs to be passed on to the next round of tracking. `terminated1` contains all tracks that have been already terminated and needs to passed on to the next round as well to make sure they don't get tracked again."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tracking_parameters_split['start_time'] = '1976-01-18'\n",
    "tracking_parameters_split['end_time'] =  '1976-01-31'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tracks_from_split, tracks_split2, terminated2 = et.tracking.split_track(tracking_parameters_split, \n",
    "                                                                        in_file=False, continuing=True,\n",
    "                                                                        tracks=tracks_split1,\n",
    "                                                                        terminated_set=terminated1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tracks_from_split` now contains the same tracks as the variable `tracks` from the normal tracking function `track()`. `tracks_split2` and `terminated2` are only needed when another period of tracking is to be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note for the setting of `sample_parameters`:  \n",
    "1. `'start_time'` is required to be no earlier than the earliest actual date of the tracked eddies. In our case here, for the year 1976 and a 5-day temporal resolution of the data, this is `'1976-01-03'` (if we had daily data, it would be `'1976-01-01'`).\n",
    "2. `'lon1'` and `'lon2'` need to be identical to `'lon1'` and `'lon2'` in `detection_parameters`.  \n",
    "3. Right now, the usage of `'range'` and `'split'` has not been thouroughly tested! It seems to work for most cases though.  \n",
    "\n",
    "`'range'`: Set to `True` if you only want to sample eddies within a certain range `'values_range'` of a property `'var_range'` in the dataset `'ds_range'`. `'var_range'` needs to be 2D (thus the `.isel(z=9)` in the example below) and interpolated to the same grid as `OW` used above. It is most likely that, if you follow this example, `'var_range'` is stored in the same dataset as `OW`. In the example below, only eddies that have a center temperature between 4 and 7 degrees C at depth level 10 (`z=9`) will be sampled and stored.  \n",
    "\n",
    "`'split'`: Set to `True` if you want to split the sampled eddies into two categories, above and below a certain threshold value `'value_split'` of a variable `'var_split'` in the dataset `'ds_split'`. As for `'range'`, `'var_split'` needs to be 2D and interpolated to the same grid as `OW` used above. In the example below the eddies will be put into two categories: In the first category, the eddies must have a center surface salinity above 34.0 and in the second category, below 34.0."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For the sampling we again have to specify some parameters, defining when, where and which \n",
    "# eddies to sample.\n",
    "# Because the dataset containing the sampled eddies can grow huge for larger regions and/or\n",
    "# longer time periods, `eddytools.sample.sample()` writes the samples for each individual\n",
    "# eddy to individual netcdf-files on disk!\n",
    "sample_parameters = {'model': 'ORCA',\n",
    "                     'grid': 'latlon',\n",
    "                     'start_time': '1976-01-03', # time range start\n",
    "                     'end_time': '1976-12-31', # time range end\n",
    "                     'calendar': 'standard', # calendar, must be either 360_day or standard\n",
    "                     'max_time': 73, # maximum length of tracks to consider\n",
    "                                     # (model time steps)\n",
    "                     'lon1': 172, # minimum longitude of detection region\n",
    "                     'lon2': -178, # maximum longitude\n",
    "                     'lat1': -58, # minimum latitude\n",
    "                     'lat2': -52, # maximum latitude\n",
    "                     'type': 'anticyclonic', # type of eddy\n",
    "                     'lifetime': 5, # length of the eddy's track in days\n",
    "                     'size': 25, # eddy size (radius in km)\n",
    "                     'd_surr': 1, # surroundings size\n",
    "                     'range': False, # sample eddy within a range of `var_range`\n",
    "                     'ds_range': data_int.isel(z=9), # dataset of `var_range`\n",
    "                     'var_range': ['votemper'], # variable to base the range on\n",
    "                     'value_range': [[4, 7],], # range of `var_range`\n",
    "                     'split': False, # split eddies at a threshold in below and above\n",
    "                     'ds_split': data_int.isel(z=0), # dataset of `var_split`\n",
    "                     'var_split': ['vosaline'], # variable to base split on\n",
    "                     'value_split': [34.0,], # split eddies at this value\n",
    "                     'sample_vars': ['votemper'], # variables to sample\n",
    "                     'save_location': datapath, # where to store the netcdf files \n",
    "                     'save_name': 'test'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling can take quite long as for every eddy that fits the criteria we need to read data from disk at every time step  \n",
    "The names of the files will be defined by `save_name`, the criteria you specify and the eddy number  \n",
    "For this sampling parameters, the file name of the first eddy will be  \n",
    "`test.anticyclonic.larger_25.longer_5.0000001.nc`  \n",
    "\n",
    "If you set `'split'` to `True`, the file names will differ for the two categories.  \n",
    "`test.anticyclonic.larger_25.longer_5.0000001.above_thr.nc` for eddies that are above `'value_split'`  \n",
    "`test.anticyclonic.larger_25.longer_5.0000001.below_thr.nc`\n",
    "for eddies that are below `'value_split'`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "et.sample.sample(tracks, data_int, sample_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# AVERAGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now average over the sampled eddies.  \n",
    "As each eddy has its own file, we first need to find out how many files/samples there are, so we can loop over them and then store the datasets in a dictionary.  \n",
    "Note that for large region, i.e. a lot of samples the resulting dataset could be too large to fit into memory. The `chunks={}` argument to `xr.open_dataset()` is an attempt to overcome this, however I do not know yet whether this has a lot of effect! One could also split the samples into several parts and then work on each part seperately (the number of eddies going into each average is stored, so one could later do a weighted average over the different parts!)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Find out number of samples\n",
    "samples = {}\n",
    "num_samples = len(glob(sample_parameters['save_location']\n",
    "                       + sample_parameters['save_name'] + '.anticyclonic.larger_25.longer_5.*.nc'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now we load each stored sample\n",
    "for i in np.arange(1, num_samples + 1):\n",
    "    snum = \"%07d\" % (i,)\n",
    "    samples[i] = xr.open_dataset(sample_parameters['save_location'] + sample_parameters['save_name'] +\n",
    "                                 '.anticyclonic.larger_25.longer_5.' + str(snum) + '.nc', chunks={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a \"preparation\" is performed: Basically, all eddies are interpolated onto a normalized (in length) section crossing them through the eddy center. This section's orientation can be defined by the argument `section`, which can be either `'zonal'` (default) or `'meridional'`. The length of this section can be defined with argument `interp_vec`. A larger number gives a finer resolution of the interpolated data, but might not necessarily be useful (if input data is coarser resolution for example).    \n",
    "For every variable specified, at every time step, the values and anomalies (with respect to surroundings) are interpolated (with method `method`) onto the normalized section and stored according to the month the eddy was first detected. The depth profile of the surroundings is stored as well.  \n",
    "For available interpolation methods please have a look in the documentation of the underlying function [`scipy.interpolate.interp1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "normalized_eddies = et.average.prepare(samples, ['votemper'], interp_vec=41, interp_method='nearest', section='zonal')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "normalized_eddies['votemper_anom']['07'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for example, 5 eddies originating in July have been stored with a maximum length of 37 time steps. 11 is the length of the depth dimension, 41 the length of the normalized section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can average these samples in different ways.  \n",
    "Three possibilities are given with `average`  \n",
    "1. seasonal -> bins the samples (of all available years) into four seasons (DJF, MAM, JJA, SON) and averages them into a seasonal climatology  \n",
    "2. monthly  -> does the same for each month (so you end up with a monthly climatology)  \n",
    "3. total    -> averages over all available eddies  \n",
    "The three methods all return the means, standard deviations (across eddies), and the number of eddies that went into the derived quantities for a) the variables specified, b) their anomalies to the surroundings, c) the surroundings. These results are stored in the output dictionary under `output['ave']['mean'][period]['variable']`, `output['ave']['mean'][period]['variable_anom']`, `output['ave']['mean'][period]['variable_around']`, respectively. `period` refers to 1. `DJF`, `MAM`, `JJA` or `SON`, 2. `01`, `02`, `03`, etc., representing the month, or in case of the total average thie layer in the dictionary does not exist.  \n",
    "Additionally, the averaged temporal evolution of the eddy centers will be stored under `output['evo'][...]`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "season_eddies = et.average.seasonal(normalized_eddies, ['votemper'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.pcolormesh(season_eddies['ave']['mean']['DJF']['votemper_anom'], vmin=-1, vmax=1, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "monthly_eddies = et.average.monthly(normalized_eddies, ['votemper'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.pcolormesh(monthly_eddies['ave']['mean']['03']['votemper_anom'], vmin=-1, vmax=1, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_eddies = et.average.total(normalized_eddies, ['votemper'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.pcolormesh(total_eddies['ave']['mean']['votemper_anom'], vmin=-1, vmax=1, cmap=cm.balance)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-eddytools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
